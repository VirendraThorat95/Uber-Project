{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b40680",
   "metadata": {},
   "source": [
    "# Aggregated & Preprocessed Data â€” EDA Notebook\n",
    "\n",
    "**Author:** Virendra\n",
    "\n",
    "## Overview\n",
    "This notebook performs exploratory data analysis (EDA), data cleaning, preprocessing, aggregation, and feature engineering on the Uber traffic and weather datasets. The notebook is written in a **very detailed** style (Option A) with explanatory markdown cells for each step to help reviewers understand the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71309bf2",
   "metadata": {},
   "source": [
    "## 1. Environment & Libraries\n",
    "\n",
    "Import required Python libraries. If you don't have any of these installed, run `pip install` or use conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Display settings for pandas\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "print('Python:', sys.version.splitlines()[0])\n",
    "print('Pandas:', pd.__version__)\n",
    "print('NumPy:', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71989c60",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data\n",
    "\n",
    "Load the traffic and merged datasets from the specified file paths. If the merged file exists, we'll load it; otherwise we'll load the raw traffic file and fetch/merge weather later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49807f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (from user)\n",
    "uber_path = r\"C:\\Users\\dell8\\Downloads\\Uber Project\\Dataset_Uber Traffic.csv\"\n",
    "merged_path = r\"C:\\Desktop\\Upgrad_Python\\Uber Project\\merged_uber_weather_lag.csv\"\n",
    "\n",
    "print('Uber path ->', uber_path)\n",
    "print('Merged path ->', merged_path)\n",
    "\n",
    "# Load files (use merged if present)\n",
    "if os.path.exists(merged_path):\n",
    "    print('Loading merged dataset...')\n",
    "    df = pd.read_csv(merged_path)\n",
    "    used_merged = True\n",
    "else:\n",
    "    print('Merged not found. Loading raw traffic dataset...')\n",
    "    traffic = pd.read_csv(uber_path)\n",
    "    used_merged = False\n",
    "    df = traffic.copy()\n",
    "\n",
    "print('\\nDataframe shape:', df.shape)\n",
    "print('\\nColumns:')\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Show top rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b1199b",
   "metadata": {},
   "source": [
    "## 3. Initial Inspection\n",
    "\n",
    "Check datatypes, missing values, and a quick summary. This helps identify issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(df.info())\n",
    "\n",
    "# Summary stats for numeric columns\n",
    "print('\\nNumeric summary:')\n",
    "print(df.describe(include='all').transpose().round(3))\n",
    "\n",
    "# Missing values\n",
    "print('\\nMissing values count:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a51850",
   "metadata": {},
   "source": [
    "## 4. DateTime parsing and correction\n",
    "\n",
    "Ensure the DateTime column is parsed properly and set correct dtype. We also create time-based features: hour, day_of_week, month, is_weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c53c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'DateTime' in df.columns:\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'], dayfirst=True, errors='coerce')\n",
    "else:\n",
    "    # try some common alternatives\n",
    "    for c in df.columns:\n",
    "        if 'date' in c.lower() or 'time' in c.lower():\n",
    "            df.rename(columns={c: 'DateTime'}, inplace=True)\n",
    "            df['DateTime'] = pd.to_datetime(df['DateTime'], dayfirst=True, errors='coerce')\n",
    "            break\n",
    "\n",
    "print(df['DateTime'].head())\n",
    "\n",
    "# Time features\n",
    "if 'DateTime' in df.columns:\n",
    "    df['hour'] = df['DateTime'].dt.hour\n",
    "    df['day_of_week'] = df['DateTime'].dt.dayofweek\n",
    "    df['month'] = df['DateTime'].dt.month\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "print(df[['DateTime','hour','day_of_week','month','is_weekend']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e6150",
   "metadata": {},
   "source": [
    "## 5. Missing Values Strategy\n",
    "\n",
    "- Show columns with missing values\n",
    "- Apply forward/backward fill for weather features if present\n",
    "- For traffic `Vehicles` consider removing or imputing (we'll forward/backward fill as a safe approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing = missing[missing>0].sort_values(ascending=False)\n",
    "missing\n",
    "\n",
    "# Example imputation strategy\n",
    "weather_cols = [c for c in df.columns if c.lower() in ['temp','rhum','wspd','prcp','precip','temperature']]\n",
    "print('Detected weather-like columns:', weather_cols)\n",
    "\n",
    "# Fill strategy\n",
    "if weather_cols:\n",
    "    df[weather_cols] = df[weather_cols].ffill().bfill()\n",
    "\n",
    "if 'Vehicles' in df.columns:\n",
    "    df['Vehicles'] = df['Vehicles'].ffill().bfill()\n",
    "\n",
    "# Drop rows where DateTime is missing (cannot work without timestamp)\n",
    "df = df.dropna(subset=['DateTime'])\n",
    "\n",
    "print('\\nMissing values after imputation:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Keep a copy of preprocessed dataframe\n",
    "df_preprocessed = df.copy()\n",
    "\n",
    "print('Done imputation and cleanup. Shape ->', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e3a9d",
   "metadata": {},
   "source": [
    "## 6. Remove Duplicates & Correct Data Types\n",
    "\n",
    "- Remove exact duplicates\n",
    "- Ensure numeric columns are numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be56db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "after = df.shape[0]\n",
    "print(f'Removed {before-after} duplicate rows')\n",
    "\n",
    "# Convert numeric columns\n",
    "num_cols = ['Vehicles','temp','rhum','wspd','prcp']\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "print('Types after numeric conversion:')\n",
    "print(df.dtypes[df.dtypes!='object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d530783",
   "metadata": {},
   "source": [
    "## 7. Aggregation: Hourly traffic per Junction\n",
    "\n",
    "If traffic is already hourly aggregated this will keep it; otherwise we aggregate (sum) Vehicles per hour per junction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure DateTime is rounded/truncated to hour\n",
    "if 'DateTime' in df.columns:\n",
    "    df['DateTime_hour'] = df['DateTime'].dt.floor('H')\n",
    "\n",
    "agg_cols = ['DateTime_hour','Junction']\n",
    "if 'Vehicles' in df.columns:\n",
    "    agg_df = df.groupby(agg_cols)['Vehicles'].sum().reset_index().rename(columns={'DateTime_hour':'DateTime'})\n",
    "else:\n",
    "    agg_df = df.copy()\n",
    "\n",
    "print('Aggregated shape:', agg_df.shape)\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53634b4e",
   "metadata": {},
   "source": [
    "## 8. Merge with Weather Data (if available)\n",
    "\n",
    "If the merged dataset wasn't available earlier, attempt to load a weather file or merged file to attach weather features. If your `merged_uber_weather_lag.csv` exists we will use it as canonical merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38338e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user provided merged_path earlier and it exists, load it to get weather\n",
    "if os.path.exists(merged_path):\n",
    "    print('Loading user provided merged dataset to ensure weather features are present')\n",
    "    merged_df = pd.read_csv(merged_path)\n",
    "    # Try to ensure columns align\n",
    "    if 'DateTime' in merged_df.columns:\n",
    "        merged_df['DateTime'] = pd.to_datetime(merged_df['DateTime'], dayfirst=True, errors='coerce')\n",
    "    print('Merged dataset shape:', merged_df.shape)\n",
    "    display_cols = [c for c in merged_df.columns if c in ['DateTime','Junction','Vehicles','temp','rhum','wspd','prcp']]\n",
    "    print('Sample from merged dataset:')\n",
    "    print(merged_df[display_cols].head())\n",
    "else:\n",
    "    print('No merged dataset found at given path; ensure you have the weather file separately and merge manually.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc50bb",
   "metadata": {},
   "source": [
    "## 9. Feature Engineering\n",
    "\n",
    "Create time-based features, lag features, rolling-window statistics, and event/weekend flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f410bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with agg_df (hourly traffic)\n",
    "if 'agg_df' in globals():\n",
    "    df_features = agg_df.copy()\n",
    "else:\n",
    "    df_features = df.copy()\n",
    "\n",
    "# Ensure DateTime\n",
    "if 'DateTime' in df_features.columns:\n",
    "    df_features['DateTime'] = pd.to_datetime(df_features['DateTime'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Time features\n",
    "if 'DateTime' in df_features.columns:\n",
    "    df_features['hour'] = df_features['DateTime'].dt.hour\n",
    "    df_features['day_of_week'] = df_features['DateTime'].dt.dayofweek\n",
    "    df_features['month'] = df_features['DateTime'].dt.month\n",
    "    df_features['is_weekend'] = df_features['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "# Lag features per junction\n",
    "if 'Junction' in df_features.columns and 'Vehicles' in df_features.columns:\n",
    "    df_features = df_features.sort_values(['Junction','DateTime'])\n",
    "    for lag in [1,2,3]:\n",
    "        df_features[f'Vehicles_lag_{lag}h'] = df_features.groupby('Junction')['Vehicles'].shift(lag)\n",
    "\n",
    "# Rolling features: if weather columns exist in merged_df, map them to df_features by DateTime+Junction\n",
    "weather_cols = ['temp','rhum','wspd','prcp']\n",
    "if os.path.exists(merged_path):\n",
    "    merged_df = pd.read_csv(merged_path)\n",
    "    merged_df['DateTime'] = pd.to_datetime(merged_df['DateTime'], dayfirst=True, errors='coerce')\n",
    "    # merge on DateTime and Junction if both exist\n",
    "    if 'Junction' in merged_df.columns and 'Junction' in df_features.columns:\n",
    "        df_features = pd.merge(df_features, merged_df[['DateTime','Junction'] + [c for c in weather_cols if c in merged_df.columns]], on=['DateTime','Junction'], how='left')\n",
    "\n",
    "# Compute rolling means per junction for weather\n",
    "for c in weather_cols:\n",
    "    if c in df_features.columns:\n",
    "        df_features = df_features.sort_values(['Junction','DateTime'])\n",
    "        df_features[f'{c}_roll_3h'] = df_features.groupby('Junction')[c].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "        df_features[f'{c}_roll_6h'] = df_features.groupby('Junction')[c].transform(lambda x: x.rolling(window=6, min_periods=1).mean())\n",
    "\n",
    "print('Feature dataframe shape:', df_features.shape)\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c5625",
   "metadata": {},
   "source": [
    "## 10. Normalization / Standardization\n",
    "\n",
    "Scale numeric columns to have zero mean and unit variance (standardization) or scale to [0,1] (normalization) depending on modeling needs. We'll demonstrate StandardScaler from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale_cols = [c for c in df_features.columns if c not in ['DateTime','Junction','ID'] and df_features[c].dtype in [int,float,'float64','int64']]\n",
    "# Remove boolean columns\n",
    "scale_cols = [c for c in scale_cols if df_features[c].nunique()>2]\n",
    "print('Columns to scale:', scale_cols)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "if scale_cols:\n",
    "    df_features_scaled = df_features.copy()\n",
    "    df_features_scaled[scale_cols] = scaler.fit_transform(df_features[scale_cols].fillna(0))\n",
    "else:\n",
    "    df_features_scaled = df_features.copy()\n",
    "\n",
    "print('Scaled dataframe sample:')\n",
    "df_features_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c63f6c",
   "metadata": {},
   "source": [
    "## 11. Feature Importance (Correlation + Random Forest)\n",
    "\n",
    "We'll show correlation with the target `Vehicles`, and an optional RandomForest feature importance if scikit-learn is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d514e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Correlation with Vehicles\n",
    "if 'Vehicles' in df_features_scaled.columns:\n",
    "    corr = df_features_scaled.corr()\n",
    "    if 'Vehicles' in corr.columns:\n",
    "        corr_with_target = corr['Vehicles'].sort_values(ascending=False)\n",
    "        print('Top correlations with Vehicles:')\n",
    "        print(corr_with_target.head(20))\n",
    "\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(corr.loc[corr_with_target.index[:12], corr_with_target.index[:12]], annot=True, fmt='.2f')\n",
    "        plt.title('Top feature correlations')\n",
    "        plt.show()\n",
    "\n",
    "# RandomForest feature importance (optional)\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    features = [c for c in df_features_scaled.columns if c not in ['DateTime','Junction','ID','Vehicles']]\n",
    "    features = [c for c in features if df_features_scaled[c].dtype in [int,float,'float64','int64']]\n",
    "    features = [c for c in features if df_features_scaled[c].nunique()>1]\n",
    "    print('\\nUsing features:', features[:20])\n",
    "    X = df_features_scaled[features].fillna(0)\n",
    "    y = df_features_scaled['Vehicles'].fillna(0)\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1)\n",
    "    rf.fit(X, y)\n",
    "    importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "    print('\\nTop feature importances:')\n",
    "    print(importances.head(20))\n",
    "except Exception as e:\n",
    "    print('RandomForest failed:', e)\n",
    "    print('You can install scikit-learn to run this section')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b060eb5",
   "metadata": {},
   "source": [
    "## 12. Export final processed dataset\n",
    "\n",
    "Save the final processed & feature-engineered dataset for modeling and submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f907474",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r'/mnt/data/merged_uber_weather_lag_processed.csv'\n",
    "df_features_scaled.to_csv(output_path, index=False)\n",
    "print('Saved processed dataset to:', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e2c79",
   "metadata": {},
   "source": [
    "## 13. Final Notes & Next Steps\n",
    "\n",
    "- This notebook is intentionally verbose and educational (Option A). \n",
    "- Next steps: modeling (regression/classification), hyperparameter tuning, cross-validation, and deployment.\n",
    "- If you'd like, I can also produce a condensed production-ready notebook (Option B) and a PowerPoint summarizing the EDA results."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
